<!DOCTYPE html><html lang="en"><head>
    <script src="https://www.googletagmanager.com/gtag/js?id=G-L78LX2HS2N" async=""></script>
    <script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-L78LX2HS2N');
</script>
    <link rel="stylesheet" as="style" onload="this.rel='stylesheet'" href="https://use.typekit.net/tde3xym.css">

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="shortcut icon" type="image/png" href="/innovateus_favicon.001.png">
    <link rel="stylesheet" href="https://kit.fontawesome.com/59ddbfe387.css" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/@mdi/font@5.x/css/materialdesignicons.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="module" crossorigin="" src="/assets/index.75c577c3.js"></script>
    <link rel="stylesheet" href="/assets/index.6e9d4c10.css">
  <style id="vuetify-theme-stylesheet" type="text/css">:root {
      --v-theme-background: 255,255,255;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 255,255,255;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 255,255,255;
      --v-theme-surface-bright-overlay-multiplier: 1;
      --v-theme-surface-light: 238,238,238;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 66,66,66;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 238,238,238;
      --v-theme-primary: 24,103,192;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 31,85,146;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 72,169,166;
      --v-theme-secondary-overlay-multiplier: 1;
      --v-theme-secondary-darken-1: 1,135,134;
      --v-theme-secondary-darken-1-overlay-multiplier: 1;
      --v-theme-error: 176,0,32;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 1;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 1;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 1;
      --v-theme-on-background: 0,0,0;
      --v-theme-on-surface: 0,0,0;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 0,0,0;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 0, 0, 0;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 0.87;
      --v-medium-emphasis-opacity: 0.6;
      --v-disabled-opacity: 0.38;
      --v-idle-opacity: 0.04;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.12;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 33, 37, 41;
      --v-theme-on-kbd: 255, 255, 255;
      --v-theme-code: 245, 245, 245;
      --v-theme-on-code: 0, 0, 0;
    }
    .v-theme--light {
      color-scheme: normal;
      --v-theme-background: 255,255,255;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 255,255,255;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 255,255,255;
      --v-theme-surface-bright-overlay-multiplier: 1;
      --v-theme-surface-light: 238,238,238;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 66,66,66;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 238,238,238;
      --v-theme-primary: 24,103,192;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 31,85,146;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 72,169,166;
      --v-theme-secondary-overlay-multiplier: 1;
      --v-theme-secondary-darken-1: 1,135,134;
      --v-theme-secondary-darken-1-overlay-multiplier: 1;
      --v-theme-error: 176,0,32;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 1;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 1;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 1;
      --v-theme-on-background: 0,0,0;
      --v-theme-on-surface: 0,0,0;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 0,0,0;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 0, 0, 0;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 0.87;
      --v-medium-emphasis-opacity: 0.6;
      --v-disabled-opacity: 0.38;
      --v-idle-opacity: 0.04;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.12;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 33, 37, 41;
      --v-theme-on-kbd: 255, 255, 255;
      --v-theme-code: 245, 245, 245;
      --v-theme-on-code: 0, 0, 0;
    }
    .v-theme--dark {
      color-scheme: dark;
      --v-theme-background: 18,18,18;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 33,33,33;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 204,191,214;
      --v-theme-surface-bright-overlay-multiplier: 2;
      --v-theme-surface-light: 66,66,66;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 200,200,200;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 0,0,0;
      --v-theme-primary: 33,150,243;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 39,124,193;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 84,182,178;
      --v-theme-secondary-overlay-multiplier: 2;
      --v-theme-secondary-darken-1: 72,169,166;
      --v-theme-secondary-darken-1-overlay-multiplier: 2;
      --v-theme-error: 207,102,121;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 2;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 2;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 2;
      --v-theme-on-background: 255,255,255;
      --v-theme-on-surface: 255,255,255;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 255,255,255;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 255, 255, 255;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 1;
      --v-medium-emphasis-opacity: 0.7;
      --v-disabled-opacity: 0.5;
      --v-idle-opacity: 0.1;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.16;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 33, 37, 41;
      --v-theme-on-kbd: 255, 255, 255;
      --v-theme-code: 52, 52, 52;
      --v-theme-on-code: 204, 204, 204;
    }
    .bg-background {
      --v-theme-overlay-multiplier: var(--v-theme-background-overlay-multiplier);
      background-color: rgb(var(--v-theme-background)) !important;
      color: rgb(var(--v-theme-on-background)) !important;
    }
    .bg-surface {
      --v-theme-overlay-multiplier: var(--v-theme-surface-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface)) !important;
      color: rgb(var(--v-theme-on-surface)) !important;
    }
    .bg-surface-bright {
      --v-theme-overlay-multiplier: var(--v-theme-surface-bright-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-bright)) !important;
      color: rgb(var(--v-theme-on-surface-bright)) !important;
    }
    .bg-surface-light {
      --v-theme-overlay-multiplier: var(--v-theme-surface-light-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-light)) !important;
      color: rgb(var(--v-theme-on-surface-light)) !important;
    }
    .bg-surface-variant {
      --v-theme-overlay-multiplier: var(--v-theme-surface-variant-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-variant)) !important;
      color: rgb(var(--v-theme-on-surface-variant)) !important;
    }
    .bg-primary {
      --v-theme-overlay-multiplier: var(--v-theme-primary-overlay-multiplier);
      background-color: rgb(var(--v-theme-primary)) !important;
      color: rgb(var(--v-theme-on-primary)) !important;
    }
    .bg-primary-darken-1 {
      --v-theme-overlay-multiplier: var(--v-theme-primary-darken-1-overlay-multiplier);
      background-color: rgb(var(--v-theme-primary-darken-1)) !important;
      color: rgb(var(--v-theme-on-primary-darken-1)) !important;
    }
    .bg-secondary {
      --v-theme-overlay-multiplier: var(--v-theme-secondary-overlay-multiplier);
      background-color: rgb(var(--v-theme-secondary)) !important;
      color: rgb(var(--v-theme-on-secondary)) !important;
    }
    .bg-secondary-darken-1 {
      --v-theme-overlay-multiplier: var(--v-theme-secondary-darken-1-overlay-multiplier);
      background-color: rgb(var(--v-theme-secondary-darken-1)) !important;
      color: rgb(var(--v-theme-on-secondary-darken-1)) !important;
    }
    .bg-error {
      --v-theme-overlay-multiplier: var(--v-theme-error-overlay-multiplier);
      background-color: rgb(var(--v-theme-error)) !important;
      color: rgb(var(--v-theme-on-error)) !important;
    }
    .bg-info {
      --v-theme-overlay-multiplier: var(--v-theme-info-overlay-multiplier);
      background-color: rgb(var(--v-theme-info)) !important;
      color: rgb(var(--v-theme-on-info)) !important;
    }
    .bg-success {
      --v-theme-overlay-multiplier: var(--v-theme-success-overlay-multiplier);
      background-color: rgb(var(--v-theme-success)) !important;
      color: rgb(var(--v-theme-on-success)) !important;
    }
    .bg-warning {
      --v-theme-overlay-multiplier: var(--v-theme-warning-overlay-multiplier);
      background-color: rgb(var(--v-theme-warning)) !important;
      color: rgb(var(--v-theme-on-warning)) !important;
    }
    .text-background {
      color: rgb(var(--v-theme-background)) !important;
    }
    .border-background {
      --v-border-color: var(--v-theme-background);
    }
    .text-surface {
      color: rgb(var(--v-theme-surface)) !important;
    }
    .border-surface {
      --v-border-color: var(--v-theme-surface);
    }
    .text-surface-bright {
      color: rgb(var(--v-theme-surface-bright)) !important;
    }
    .border-surface-bright {
      --v-border-color: var(--v-theme-surface-bright);
    }
    .text-surface-light {
      color: rgb(var(--v-theme-surface-light)) !important;
    }
    .border-surface-light {
      --v-border-color: var(--v-theme-surface-light);
    }
    .text-surface-variant {
      color: rgb(var(--v-theme-surface-variant)) !important;
    }
    .border-surface-variant {
      --v-border-color: var(--v-theme-surface-variant);
    }
    .on-surface-variant {
      color: rgb(var(--v-theme-on-surface-variant)) !important;
    }
    .text-primary {
      color: rgb(var(--v-theme-primary)) !important;
    }
    .border-primary {
      --v-border-color: var(--v-theme-primary);
    }
    .text-primary-darken-1 {
      color: rgb(var(--v-theme-primary-darken-1)) !important;
    }
    .border-primary-darken-1 {
      --v-border-color: var(--v-theme-primary-darken-1);
    }
    .text-secondary {
      color: rgb(var(--v-theme-secondary)) !important;
    }
    .border-secondary {
      --v-border-color: var(--v-theme-secondary);
    }
    .text-secondary-darken-1 {
      color: rgb(var(--v-theme-secondary-darken-1)) !important;
    }
    .border-secondary-darken-1 {
      --v-border-color: var(--v-theme-secondary-darken-1);
    }
    .text-error {
      color: rgb(var(--v-theme-error)) !important;
    }
    .border-error {
      --v-border-color: var(--v-theme-error);
    }
    .text-info {
      color: rgb(var(--v-theme-info)) !important;
    }
    .border-info {
      --v-border-color: var(--v-theme-info);
    }
    .text-success {
      color: rgb(var(--v-theme-success)) !important;
    }
    .border-success {
      --v-border-color: var(--v-theme-success);
    }
    .text-warning {
      color: rgb(var(--v-theme-warning)) !important;
    }
    .border-warning {
      --v-border-color: var(--v-theme-warning);
    }
    .on-background {
      color: rgb(var(--v-theme-on-background)) !important;
    }
    .on-surface {
      color: rgb(var(--v-theme-on-surface)) !important;
    }
    .on-surface-bright {
      color: rgb(var(--v-theme-on-surface-bright)) !important;
    }
    .on-surface-light {
      color: rgb(var(--v-theme-on-surface-light)) !important;
    }
    .on-primary {
      color: rgb(var(--v-theme-on-primary)) !important;
    }
    .on-primary-darken-1 {
      color: rgb(var(--v-theme-on-primary-darken-1)) !important;
    }
    .on-secondary {
      color: rgb(var(--v-theme-on-secondary)) !important;
    }
    .on-secondary-darken-1 {
      color: rgb(var(--v-theme-on-secondary-darken-1)) !important;
    }
    .on-error {
      color: rgb(var(--v-theme-on-error)) !important;
    }
    .on-info {
      color: rgb(var(--v-theme-on-info)) !important;
    }
    .on-success {
      color: rgb(var(--v-theme-on-success)) !important;
    }
    .on-warning {
      color: rgb(var(--v-theme-on-warning)) !important;
    }
</style><title>RebootDemocracy.AI Blog | News That Caught Our Eye #37: November 21, 2024</title><meta name="title" content="RebootDemocracy.AI Blog | News That Caught Our Eye #37: November 21, 2024"><meta name="description" content="This Week in AI News: Frank McCourt, speaking at the Rebooting Democracy in the Age of AI Lecture Series, highlighted the dangers of Big Tech's centralized control over personal data and proposed a &quot;New Net&quot; to prioritize user-owned identities and data portability. The National League of Cities, in partnership with Google, released a toolkit to guide local governments in adopting AI responsibly, emphasizing public engagement and tailored policies. The Texas Legislature is set to consider the Responsible AI Governance Act, which would regulate high-risk AI systems and create an AI Regulatory Sandbox Program. The U.S. Department of Homeland Security unveiled a framework for safely deploying AI in critical infrastructure sectors like energy and transportation. Additionally, the U.S. Patent and Trademark Office banned generative AI tools for most tasks, citing concerns over security and bias, while exploring AI's potential for internal testing and patent searches. From local governance to federal oversight, these developments reflect ongoing efforts to balance AI innovation with ethical and secure implementation."><meta property="og:title" content="RebootDemocracy.AI Blog | News That Caught Our Eye #37: November 21, 2024"><meta property="og:type" content="website"><meta property="og:url" content="https://rebootdemocracy.ai/blog/News-That-Caught-Our-Eye-November-21"><meta property="og:description" content="This Week in AI News: Frank McCourt, speaking at the Rebooting Democracy in the Age of AI Lecture Series, highlighted the dangers of Big Tech's centralized control over personal data and proposed a &quot;New Net&quot; to prioritize user-owned identities and data portability. The National League of Cities, in partnership with Google, released a toolkit to guide local governments in adopting AI responsibly, emphasizing public engagement and tailored policies. The Texas Legislature is set to consider the Responsible AI Governance Act, which would regulate high-risk AI systems and create an AI Regulatory Sandbox Program. The U.S. Department of Homeland Security unveiled a framework for safely deploying AI in critical infrastructure sectors like energy and transportation. Additionally, the U.S. Patent and Trademark Office banned generative AI tools for most tasks, citing concerns over security and bias, while exploring AI's potential for internal testing and patent searches. From local governance to federal oversight, these developments reflect ongoing efforts to balance AI innovation with ethical and secure implementation."><meta property="og:image" content="https://burnes-center.directus.app/assets/37126544-ed3d-4f46-89a7-c5c7d61c62ac.webp"><meta property="og:image:width" content="1024"><meta property="og:image:height" content="1024"><meta property="twitter:title" content="RebootDemocracy.AI"><meta property="twitter:description" content="This Week in AI News: Frank McCourt, speaking at the Rebooting Democracy in the Age of AI Lecture Series, highlighted the dangers of Big Tech's centralized control over personal data and proposed a &quot;New Net&quot; to prioritize user-owned identities and data portability. The National League of Cities, in partnership with Google, released a toolkit to guide local governments in adopting AI responsibly, emphasizing public engagement and tailored policies. The Texas Legislature is set to consider the Responsible AI Governance Act, which would regulate high-risk AI systems and create an AI Regulatory Sandbox Program. The U.S. Department of Homeland Security unveiled a framework for safely deploying AI in critical infrastructure sectors like energy and transportation. Additionally, the U.S. Patent and Trademark Office banned generative AI tools for most tasks, citing concerns over security and bias, while exploring AI's potential for internal testing and patent searches. From local governance to federal oversight, these developments reflect ongoing efforts to balance AI innovation with ethical and secure implementation."><meta property="twitter:image" content="https://burnes-center.directus.app/assets/37126544-ed3d-4f46-89a7-c5c7d61c62ac.webp"><meta property="twitter:card" content="summary_large_image"></head>
  <body>
    
    <div id="app" class="grid justify-items-stretch" data-v-app=""><div data-v-35864f1a="" class="app-container"><div data-v-35864f1a="" class="main-content"><div class="header-section"><div class="logo-section"><div class="logo"><a href="http://thegovlab.org" target="_blank"><img src="/assets/the-govlab-logo-white.33f93d79.svg"></a><a href="http://burnes.northeastern.edu" target="_blank"><img src="/assets/burnes-logo.b722bffb.png"></a></div></div><div class="menu-component"><div><div class="menu"><a href="/about">About Us</a><a href="/blog">Blog</a><a href="/events">Events</a><div class="dropdown"><button class="dropbtn">Our Work <i class="fa-regular fa-angle-down"></i></button><div class="dropdown-content"><a href="https://thegovlab.org/beth-simone-noveck.html">About Beth Noveck</a><a href="/our-engagements">Engagements</a><a href="/our-research">Research</a><a href="https://innovate-us.org/" target="_blank">Teaching</a><a href=" https://www.publicentrepreneur.org" target="_blank">University Teaching</a><a href="/more-resources">More Resources</a></div></div><a href="/signup" class="btn btn-small btn-primary">Sign up for updates</a></div></div></div></div><div class="blog-hero"><img class="blog-img" src="https://burnes-center.directus.app/assets/37126544-ed3d-4f46-89a7-c5c7d61c62ac.webp?width=800"><div class="blog-details"><h1>News That Caught Our Eye #37: November 21, 2024</h1><p class="excerpt">This Week in AI News: Frank McCourt, speaking at the Rebooting Democracy in the Age of AI Lecture Series, highlighted the dangers of Big Tech's centralized control over personal data and proposed a "New Net" to prioritize user-owned identities and data portability. The National League of Cities, in partnership with Google, released a toolkit to guide local governments in adopting AI responsibly, emphasizing public engagement and tailored policies. The Texas Legislature is set to consider the Responsible AI Governance Act, which would regulate high-risk AI systems and create an AI Regulatory Sandbox Program. The U.S. Department of Homeland Security unveiled a framework for safely deploying AI in critical infrastructure sectors like energy and transportation. Additionally, the U.S. Patent and Trademark Office banned generative AI tools for most tasks, citing concerns over security and bias, while exploring AI's potential for internal testing and patent searches. From local governance to federal oversight, these developments reflect ongoing efforts to balance AI innovation with ethical and secure implementation.</p><p class="post-date">Published on <b>November 21, 2024</b></p><div class="hero-author-sm"><div><div class="author-item"><img class="author-headshot" src="https://burnes-center.directus.app/assets/a0478254-4344-41a9-a3b3-17673982f96d"><!----><div class="author-details"><p class="author-name">Autumn Sloboda</p><!----></div></div></div><div><div class="author-item"><!----><p class="author-no-image">D G</p><div class="author-details"><p class="author-name">Domenick Gaita</p><!----></div></div></div><div class="sm-tray"><a target="_blank" href="http://twitter.com/share?url=https://rebootdemocracy.ai/blog/News-That-Caught-Our-Eye-November-21"><i class="fa-brands fa-square-x-twitter"></i></a><a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://rebootdemocracy.ai/blog/News-That-Caught-Our-Eye-November-21"><i class="fa-brands fa-facebook"></i></a><a target="_blank" href="https://linkedin.com/shareArticle?url=https://rebootdemocracy.ai/blog/News-That-Caught-Our-Eye-November-21&amp;title=News That Caught Our Eye #37: November 21, 2024"><i class="fa-brands fa-linkedin"></i></a><a target="_blank" href="https://bsky.app/intent/compose?text=https://rebootdemocracy.ai/blog/News-That-Caught-Our-Eye-November-21"><i class="fa-brands fa-square-bluesky"></i></a></div></div></div></div><div class="blog-body"><div class="audio-version"><p dir="ltr"><em>Listen to the AI-generated audio version of this piece.&nbsp;</em></p><p><audio controls=""><source src="https://burnes-center.directus.app/assets/50249e8a-a521-4b7d-a441-c77ed650b393" type="audio/mpeg" data-mce-fragment="1"></audio></p></div><div class="blog-content"><h2 dir="ltr">Public AI</h2>
<p dir="ltr"><a href="https://rebootdemocracy.ai/blog/frank-mccourt-on-our-biggest-fight-and-future-of-tiktok">Frank McCourt on Our Biggest Fight and Future of TikTok</a> - Reboot Democracy, By Beth Simone Noveck, November 20, 2024</p>
<p dir="ltr">The MIT Tech Review profile on biometric company CLEAR highlights the growing concern over surrendering personal data for convenience, a theme echoed by Frank McCourt, founder of Project Liberty and author of Our Biggest Fight. Speaking at the Rebooting Democracy in the Age of AI Lecture Series, McCourt criticized the centralized control of data by tech giants, likening it to a "feudal Internet" where platforms act as modern monarchs. He advocates for a "New Net" with user-owned identities, portable data, and value-driven engagement, using a potential TikTok acquisition to showcase this vision. While his approach emphasizes personal data sovereignty, critics argue that collective governance and democratic institutions are essential for tackling shared digital challenges like misinformation, content moderation, and algorithmic bias. McCourt calls for bipartisan collaboration to ensure technology serves humanity, balancing individual rights with societal needs.<strong><br></strong></p>
<h2 dir="ltr">AI And Public Engagement</h2>
<p dir="ltr"><a href="https://royalsocietypublishing.org/doi/full/10.1098/rsta.2024.0113">Co-creating the future: participatory cities and digital governance</a> - Royal Society Publishing, Dirk Helbing et al, November 13, 2024</p>
<p dir="ltr">“The digital revolution, fueled by advancements in social media, Big Data, the Internet of Things and Artificial Intelligence, is reshaping our urban landscapes into ‘participatory cities’. These cities leverage digital technologies to foster citizen engagement, collaborative decision-making and community-driven urban development, thus unlocking new potentials while confronting emerging threats. Such technologies are empowering individuals and organizations in ways that were unimaginable just a few years ago. They do, however, introduce new risks and vulnerabilities that must be carefully managed. Hence, socio-technical innovation is urgently needed. In this connection, open-source technologies, participatory approaches and new forms of governance are becoming more popular and relevant. This theme issue looks into the tangible impacts of these technological advancements, with a focus on participatory cities. It aims to explain how digital tools are used in cities to tackle urban challenges, improve governance and promote sustainability. Through a collection of in-depth analyses, case studies and real-world examples, this issue seeks to offer a comprehensive understanding of the digital governance frameworks underpinning participatory cities. By offering a platform for multidisciplinary discourse, this theme issue endeavors to contribute to the broader narrative of shaping a more resilient, sustainable and democratic urban future in the digital age.”</p>
<p dir="ltr"><a href="https://link.springer.com/article/10.1007/s10676-024-09811-4">Procedural fairness in algorithmic decision-making: the role of public engagement</a> - Springer Nature Link, Marie Christin Decker, November 15, 2024</p>
<p dir="ltr">“Despite the widespread use of automated decision-making (ADM) systems, they are often developed without involving the public or those directly affected, leading to concerns about systematic biases that may perpetuate structural injustices. Existing formal fairness approaches primarily focus on statistical outcomes across demographic groups or individual fairness, yet these methods reveal ambiguities and limitations in addressing fairness comprehensively. This paper argues for a holistic approach to algorithmic fairness that integrates procedural fairness, considering both decision-making processes and their outcomes. Procedural fairness emphasizes the importance of fair decision-making procedures, which aligns with theories of relational justice that stress the quality of social relations and power dynamics. We highlight the need for substantive procedural fairness to ensure better outcomes and address forward-looking responsibilities. Additionally, we propose leveraging Public Engagement, a core dimension within the well-established Responsible Research and Innovation framework, to enhance procedural fairness in ADM systems. Our contribution underscores the value of Public Engagement in fostering fairer ADM processes, thereby expanding the current focus beyond technical outcome-based approaches to encompass broader procedural considerations.”</p>
<p dir="ltr"><a href="https://www.sciencedirect.com/science/article/pii/S0004370224001802?via%3Dihub">Human-AI coevolution</a> - ScienceDirect, February 2024</p>
<p dir="ltr">“Human-AI coevolution, defined as a process in which humans and AI algorithms continuously influence each other, increasingly characterizes our society, but is understudied in artificial intelligence and complexity science literature. Recommender systems and assistants play a prominent role in human-AI coevolution, as they permeate many facets of daily life and influence human choices through online platforms. The interaction between users and AI results in a potentially endless feedback loop, wherein users' choices generate data to train AI models, which, in turn, shape subsequent user preferences. This human-AI feedback loop has peculiar characteristics compared to traditional human-machine interaction and gives rise to complex and often “unintended” systemic outcomes. This paper introduces human-AI coevolution as the cornerstone for a new field of study at the intersection between AI and complexity science focused on the theoretical, empirical, and mathematical investigation of the human-AI feedback loop. In doing so, we: (i) outline the pros and cons of existing methodologies and highlight shortcomings and potential ways for capturing feedback loop mechanisms; (ii) propose a reflection at the intersection between complexity science, AI and society; (iii) provide real-world examples for different human-AI ecosystems; and (iv) illustrate challenges to the creation of such a field of study, conceptualizing them at increasing levels of abstraction, i.e., scientific, legal and socio-political.”</p>
<h2 dir="ltr">AI for Governance</h2>
<p dir="ltr"><a href="https://www.nlc.org/resource/ai-report-and-toolkit/">AI Report and Toolkit </a>- NLC 100, November 13, 2024</p>
<p dir="ltr">Artificial Intelligence is transforming local government by improving city services, supporting staff, and enhancing decision-making. The National League of Cities (NLC) AI Report and Toolkit, developed by a 20-member AI Advisory Committee in 2024, provides practical guidance for local governments on integrating AI responsibly. The report explains various types of AI, explores their applications in city services, and includes principles for ethical use and case studies of successful implementation. The toolkit offers four key tools to help cities assess their AI readiness, develop policies, and engage the public, ensuring responsible and effective use of AI while addressing concerns about bias, privacy, and transparency.<strong><br></strong></p>
<p dir="ltr"><a href="https://fedscoop.com/ai-chatbot-part-of-federal-data-access-service/">The government is working to improve data access. An AI chatbot could be part of that</a> - FedScoop, By Madison Alder, November 18, 2024</p>
<p dir="ltr">The National Science Foundation’s demonstration project, part of the National Secure Data Service (NSDS), aims to enhance U.S. government data infrastructure, including an AI chatbot to simplify access to statistical information. The chatbot, under development with a target completion date of August 2025, could answer user queries about public data, such as STEM graduation rates or flood zones, using natural language processing and providing sources for its responses. The NSDS, mandated by the CHIPS and Science Act, seeks to streamline federal data access, improve research, and protect privacy. Current challenges include complex data linking, legal barriers, and limited accessibility. The initiative involves 30 projects, including secure computing environments, synthetic data generation, and an AI-readiness assessment for federal data. With a planned website launch in 2025 and a minimum viable product by 2027, the NSDS aims to create a seamless, user-friendly data ecosystem, testing its tools through high-impact use cases.</p>
<p dir="ltr"><a href="https://statescoop.com/to-avoid-ai-divide-national-league-of-cities-and-google-share-ai-report-and-toolkit-for-local-governments/">To avoid ‘AI divide,’ National League of Cities and Google share AI report and toolkit for local governments </a>- StateScoop, By Keely Quinlan, November 14, 2024</p>
<p dir="ltr">The National League of Cities and Google Public Sector released a report offering guidance and tools for local governments to adopt AI responsibly while avoiding an “AI divide.” The 50-page report evaluates how cities use AI, highlights its risks, and provides case studies, safety practices, and step-by-step questionnaires for AI implementation. While predictive AI has long been used by cities, generative AI is just beginning to gain traction, with 96% of surveyed mayors expressing interest. The toolkit includes readiness assessments, public engagement strategies, and policy development guides, emphasizing the need for tailored approaches and strong safeguards to modernize city services and maximize AI’s economic benefits responsibly.<strong><br></strong></p>
<p dir="ltr"><a href="https://www.brookings.edu/articles/ai-policy-directions-in-the-new-trump-administration/">AI policy directions in the new Trump administration</a> - Brookings Institution, By John Villasenor and Joshua Turner, November 15, 2024</p>
<p dir="ltr">The second Trump administration, beginning in 2025, is expected to significantly shape U.S. AI policy. Likely changes include a more hands-off approach to AI regulation, potentially rescinding or modifying Biden-era policies like the AI Executive Order, while focusing on strengthening export controls to limit China’s access to advanced AI technologies. Increased attention to AI’s military applications and a shift away from antitrust enforcement could encourage industry growth but raise competition concerns. Federal efforts may streamline state AI laws through preemption while supporting technologies like autonomous vehicles. With both opportunities and challenges ahead, the administration's policies will play a pivotal role in navigating AI's rapid advancements.</p>
<p dir="ltr"><a href="https://www.govtech.com/artificial-intelligence/resource-explores-local-government-ai-use-offers-advice">Resource Explores Local Government AI Use, Offers Advice</a> - Government Technology, November 14, 2024</p>
<p dir="ltr">The National League of Cities (NLC), in partnership with Google, released the AI in Cities report and toolkit to guide local governments in adopting artificial intelligence responsibly. The report, informed by the NLC AI Advisory Committee, provides foundational knowledge about AI, showcases responsible governance models, highlights use cases like AI-powered chatbots, and examines AI applications abroad. It includes a toolkit with four tools to help cities assess current AI use, evaluate readiness, develop policies, and engage the public. Acknowledging that each community’s needs are unique, the report emphasizes that there is no universal approach to AI adoption and encourages local leaders to tailor AI strategies to their specific challenges and goals.</p>
<h2 dir="ltr">AI and Problem Solving</h2>
<p dir="ltr"><a href="https://www.cpsai.org/pages/media/2024/11/13/ai-ready-an-evaluation-guide-for-health-and-human-services-agencies">AI-Ready: An Evaluation Guide for Health and Human Services Agencies</a> - Center for Public Sector AI, November 13, 2024</p>
<p dir="ltr">“After months of hard work, collaboration, and valuable feedback from government leaders, we are pleased to release AI Ready: An Evaluation Guide for HHS Agencies. A collaboration between the Center for Public Sector AI, U.S. Digital Response, Humane Intelligence, and the Aspen Institute Financial Security Program, this practical tool is designed to help HHS teams make informed decisions about AI adoption and proactively mitigate potential risks. This guide is the first in a series of best practices, tools, and frameworks we are building to support the safe exploration of AI in HHS service and benefit delivery.”</p>
<p dir="ltr"><a href="https://bloombergcities.jhu.edu/news/taking-ai-experimentation-next-level-cities">Taking AI experimentation to the next level in cities</a> - Bloomberg Cities, November 15, 2024</p>
<p dir="ltr">Generative AI is changing how cities work by expediting processes like procurement, citizen engagement, and service delivery. However, its transformative potential relies on city leaders actively experimenting to understand and expand its capabilities. At a recent City Innovation Studio hosted by Bloomberg Philanthropies, over 90 cities explored AI’s "jagged frontier," the boundary of what the technology can and cannot currently achieve. Leaders used AI to prototype solutions for urban challenges, uncovering its strengths in problem-solving and ideation while recognizing its limitations, such as a lack of regulatory safeguards and gaps in addressing nuanced community needs. Experts emphasize the importance of training AI with localized data, maintaining ethical oversight, and involving residents to ensure responsible adoption. With these strategies, cities can harness AI’s potential while demanding improvements from developers to better serve public needs.</p>
<p dir="ltr"><a href="https://spacenews.com/ai-takes-aim-at-tedious-tasks-in-government-contracting/">AI takes aim at tedious tasks in government contracting </a>- Space News, By Sandra Erwin, November 14, 2024</p>
<p dir="ltr">AI is transforming the traditionally time-intensive government contracting process by streamlining tasks such as identifying opportunities, drafting proposals, and analyzing compliance requirements. Various companies have developed AI-enabled tools that aim to make the procurement process more efficient while&nbsp; aligning with strict government standards. However, challenges remain, including ensuring data security and addressing potential errors. While AI accelerates processes, experts emphasize the critical role of human oversight to maintain quality and compliance, predicting that the adoption of these tools will significantly reshape the government contracting landscape in the coming years.</p>
<h2 dir="ltr">Governing AI</h2>
<p dir="ltr"><a href="https://www.wired.com/story/us-patent-trademark-office-internally-banned-generative-ai/">The US Patent and Trademark Office Banned Staff From Using Generative AI </a>- Wired, November 19, 2024</p>
<p dir="ltr">The US Patent and Trademark Office (USPTO) has banned the use of generative AI tools like ChatGPT for most tasks, citing concerns over security, bias, and unpredictability. While employees cannot rely on such tools for work or use AI-generated outputs, they are allowed to experiment with AI models within the agency's internal testing environment through its AI Lab. The USPTO has also invested $75 million to enhance its patent database with AI-powered search features. This cautious approach reflects broader challenges in integrating AI into government operations, as seen with other agencies like NASA, which limits AI use for sensitive data while experimenting with its potential for coding and data accessibility.</p>
<p dir="ltr"><a href="https://cdt.org/insights/effective-remedies-in-ai-an-insufficiently-explored-avenue-for-ai-accountability/">Effective Remedies in AI: an Insufficiently Explored Avenue for AI Accountability</a> - Center for Democracy and Technology, Laura Lazaro Cabrera, November 14, 2024</p>
<p dir="ltr">A recent article examines the ongoing challenges of ensuring effective remedies for individuals impacted by AI systems in the European Union. While the European Charter of Fundamental Rights guarantees the right to an effective remedy, the complexity and opacity of AI systems can make it difficult for individuals to understand how they have been affected, complicating legal recourse. The AI Act introduces important transparency measures, such as requiring providers to disclose the use of AI systems in decision-making, particularly for high-risk applications. Though its complaint mechanisms offer a step forward, the article highlights that further efforts, including the AI Liability Directive, are necessary to address procedural barriers and strengthen access to remedies. As AI continues to play a larger role in daily life, the article stresses the need for comprehensive legal frameworks that ensure AI accountability and protect individuals' rights across the EU.</p>
<p dir="ltr"><a href="https://www.dhs.gov/news/2024/11/14/groundbreaking-framework-safe-and-secure-deployment-ai-critical-infrastructure">Groundbreaking Framework for the Safe and Secure Deployment of AI in Critical Infrastructure Unveiled by Department of Homeland Security</a> - Department of Homeland Security, November 14, 2024</p>
<p dir="ltr">The U.S. Department of Homeland Security (DHS) has released the Roles and Responsibilities Framework for Artificial Intelligence in Critical Infrastructure. Developed by the DHS Artificial Intelligence Safety and Security Board, the framework offers guidelines for safely deploying AI across key sectors like energy, water, and transportation. It outlines recommendations for stakeholders, including AI developers, cloud providers, and infrastructure operators, to address risks such as cyberattacks, design flaws, and misuse. The voluntary framework aims to enhance AI security and collaboration across public and private sectors, promoting responsible AI integration without imposing regulatory requirements.</p>
<p dir="ltr"><a href="https://fedscoop.com/house-bill-ai-fraud-deterrence-lieu-kiley/">California lawmakers target AI-fueled fraud in new House bill</a> - FedScoop, Matt Bricken, November 15, 2024</p>
<p dir="ltr">A bipartisan pair of House lawmakers is introducing legislation to address the growing threat of AI-driven fraud. The "AI Fraud Deterrence Act," introduced by California Reps. Ted Lieu (D) and Kevin Kiley (R), aims to impose harsher penalties on individuals who use artificial intelligence to commit wire fraud, mail fraud, bank fraud, or money laundering. The bill specifically targets AI’s role in enhancing fraudulent activities, proposing to raise fines for AI-related bank fraud from $1 million to $2 million, and for AI-assisted money laundering from $500,000 to $1 million, or up to three times the value of the property involved. Penalties for wire and mail fraud would also increase from $250,000 to $1 million if AI tools are used to perpetrate the crime. In cases where fraud involves disaster aid or financial institutions, the bill would double existing penalties to $2 million, irrespective of AI involvement. The lawmakers argue that by increasing penalties for AI-fueled fraud, the bill seeks to curb the misuse of rapidly advancing AI technologies, ensuring they are used responsibly while protecting individuals and institutions from emerging threats.</p>
<p dir="ltr"><a href="https://www.insideglobaltech.com/2024/11/13/texas-legislature-to-consider-sweeping-ai-legislation-in-2025/">Texas Legislature to Consider Sweeping AI Legislation in 2025 </a>- Covington, By Matthew Shapanka, Jayne Ponder and August Gweon, November 13, 2024</p>
<p dir="ltr">Texas Representative Giovanni Capriglione has introduced a draft of the Texas Responsible AI Governance Act (TRAIGA), aiming to regulate "high-risk AI systems" with a focus on consumer protection and risk management. Modeled after Colorado's AI Act and the EU AI Act, TRAIGA includes stricter thresholds for defining high-risk systems, bans on AI systems with "unacceptable risks," and requirements for distributors to prevent algorithmic discrimination. Unique provisions include mandatory record-keeping for generative AI developers, expanded reporting obligations for deployers, and exemptions for small businesses, research, and open-source developers. TRAIGA would also create an AI Regulatory Sandbox Program, a workforce grant initiative, and an advisory AI Council, while authorizing enforcement by the Texas Attorney General. If passed, TRAIGA could serve as a national model for risk-based AI regulation, though it faces potential amendments and uncertain prospects in the upcoming 2025 legislative session.</p>
<h2 dir="ltr">AI and IR</h2>
<p dir="ltr"><a href="https://www.reuters.com/technology/artificial-intelligence/us-government-commission-pushes-manhattan-project-style-ai-initiative-2024-11-19/">US government commission pushes Manhattan Project-style AI initiative</a> - Reuters, By Anna Tong and Michael Martina, November 19, 2024</p>
<p dir="ltr">The U.S.-China Economic and Security Review Commission (USCC) has proposed a Manhattan Project-style initiative to develop artificial general intelligence (AGI) to compete with China's advancements in AI. Highlighting the importance of public-private partnerships, the report emphasizes the need for strategic government involvement without detailing specific investment strategies. Commissioner Jacob Helberg noted that addressing energy infrastructure bottlenecks, such as streamlining permits for data centers, could accelerate AI progress. In addition to AI, the USCC's annual report recommends tightening trade regulations, including ending the de minimis trade exemption, which allows duty-free entry for low-value Chinese goods, and imposing stricter controls on Chinese involvement in U.S. biotechnology. However, implementing these recommendations faces challenges, such as bipartisan disagreements and resistance from the shipping industry and trade groups concerned about e-commerce disruption.</p>
<p dir="ltr"><a href="https://www.foreignaffairs.com/united-states/war-and-peace-age-artificial-intelligence">War and Peace in the Age of Artificial Intelligence</a> - Foreign Affairs, By Henry A. Kissinger, Eric Schmidt, and Craig Mundie, November 18, 2024</p>
<p dir="ltr">AI is poised to reshape warfare, diplomacy, and global power dynamics, offering precision and speed but raising ethical and existential concerns. It could escalate conflicts by targeting digital infrastructure, fueling secrecy and paranoia among nations racing for dominance, or disrupting traditional nation-state authority as corporations and decentralized groups gain influence. While AI’s objectivity might enable better governance and conflict resolution, its lack of human restraint risks spiraling into destructive scenarios. The challenge lies in balancing AI’s potential to solve complex problems with preserving human agency and avoiding over-reliance on technology that could destabilize global order.</p>
<p dir="ltr"><a href="https://www.weforum.org/stories/2024/11/what-is-sovereign-ai-and-why-is-the-concept-so-appealing-and-fraught/">What is ‘sovereign AI’ and why is the concept so appealing (and fraught)?</a> - World Economic Forum, November 13, 2024</p>
<p dir="ltr">Countries are increasingly developing sovereign AI initiatives to protect their cultural identity, bolster economic competitiveness, and reduce reliance on foreign technologies. Denmark, for instance, has funded a supercomputer with revenues from weight-loss drugs to support AI-driven biotech and pharmaceutical research, while Italy, Sweden, the UAE, and India are pursuing similar projects. Sovereign AI enables nations to embed their values and priorities into technology but poses risks of "sovereignty traps" that could hinder global cooperation. High costs, geopolitical tensions over chip access, and the challenge of aligning national and global interests complicate efforts. Advocates suggest cross-border collaboration, like a Global AI Compact, to ensure equitable access to AI infrastructure, likening its importance to electricity in the modern world. However, nations must navigate tensions between codifying cultural identity and fostering inclusive global AI governance.</p>
<p dir="ltr"><a href="https://www.wired.com/story/treasury-outbound-investment-china-artificial-intelligence/">A US Ban on Investing in Chinese AI Startups Could Escalate Under Trump</a> - Wired, By Zeyi Yang, November 18, 2024</p>
<p dir="ltr">The US Treasury Department has finalized new restrictions, effective January 2025, limiting American investments in advanced Chinese AI startups for national security reasons. These rules ban funding for AI technologies tied to China's military or intelligence services and impose a threshold of 1025 flops (a measure of AI model power) to restrict investments in cutting-edge consumer AI. The move, part of broader efforts like export controls and investment scrutiny, aims to slow China's AI advancements while reinforcing US dominance. Although current impacts are modest, the regulations add due diligence burdens for US investors and could discourage Chinese startups from seeking US support. The incoming Trump administration may expand these measures further, with Republican China hawks advocating tougher restrictions on other sectors like biotech and batteries.</p></div><!----></div><div class="footer-section"><div class="footer-contents"><div class="logo-section footer-col"><div class="wordmark"> Reboot Democracy </div><p>A Project Of</p><a href="http://thegovlab.org" target="_blank"><img src="/assets/the-govlab-logo-white.33f93d79.svg"></a><a href="http://burnes.northeastern.edu" target="_blank"><img src="/assets/burnes-center-logo.cd8cfa81.png"></a></div><div class="footer-col"><h4>Our Work</h4><a href="/our-engagements">Equitable Engagement Lab</a><a href="https://poweratwork.us/" target="_blank">Workplace and Democracy</a><a href="../">Media and Democracy</a></div><div class="footer-col"><h4>Resources</h4><a href="/events">Reboot Democracy Lecture Series</a><a href="/our-research">Case Studies</a><a href="/our-research">Research Questions</a><a href="/our-writing">Writing</a><a href="/our-teaching">Teaching</a></div><div class="footer-col"><h4>About Reboot Democracy</h4><a href="../">Our Work</a><a href="/team">Our People</a></div></div></div><div class="cc-license"><img src="data:image/png;base64,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"><p>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p></div></div><div data-v-35864f1a="" class="chatbot-app chatbot-app-closed"><div class="bot-icon"><i class="fa-solid fa-message-bot"></i></div><!----></div></div></div>
    
  <script defer="" src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;96e0c7af8fa47552&quot;,&quot;version&quot;:&quot;2025.7.0&quot;,&quot;r&quot;:1,&quot;token&quot;:&quot;db00096689224fb9a994f8e8fd55fd5b&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfExtPri&quot;:true,&quot;cfEdge&quot;:true,&quot;cfOrigin&quot;:true,&quot;cfL4&quot;:true,&quot;cfSpeedBrain&quot;:true,&quot;cfCacheStatus&quot;:true}}}" crossorigin="anonymous"></script>


</body></html>