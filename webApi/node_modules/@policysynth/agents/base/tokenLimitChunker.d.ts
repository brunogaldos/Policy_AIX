/****************************************************************************************
 * TokenLimitChunker.ts
 * Chunk‑then‑summarise helper that retries large prompts when the model rejects them
 * for context‑window overflow.  Revised July 2025.
 ****************************************************************************************/
import { BaseChatModel } from "../aiModels/baseChatModel.js";
import { PsAiModelType, PsAiModelSize } from "../aiModelTypes.js";
import { PolicySynthAgentBase } from "./agentBase.js";
export interface ModelCaller {
    callModel(type: PsAiModelType, size: PsAiModelSize, messages: PsModelMessage[], options: PsCallModelOptions): Promise<any>;
}
export declare class TokenLimitChunker extends PolicySynthAgentBase {
    private readonly manager;
    constructor(manager: ModelCaller);
    private static geminiAi;
    private static geminiTokenCount;
    private countTokens;
    static isTokenLimitError(err: unknown): boolean;
    /** Extract the model's context-window size (tokens) from a provider error msg. */
    static parseTokenLimit(err: any): number | undefined;
    private calcTokenLimitFromModel;
    chunkByTokensGemini(model: BaseChatModel, text: string, allowedTokens: number): Promise<string[]>;
    private chunkByTokens;
    handle(model: BaseChatModel, modelType: PsAiModelType, modelSize: PsAiModelSize, messages: PsModelMessage[], options: PsCallModelOptions, err: any): Promise<any>;
}
//# sourceMappingURL=tokenLimitChunker.d.ts.map